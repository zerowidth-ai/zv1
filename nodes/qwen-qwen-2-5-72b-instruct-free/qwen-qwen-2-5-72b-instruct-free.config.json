{
  "display_name": "Qwen2.5 72B Instruct (free)",
  "description": "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
  "category": "llm",
  "provider": "qwen",
  "accepts_plugins": false,
  "model_id": "qwen/qwen-2.5-72b-instruct:free",
  "context_length": 32768,
  "supported_parameters": [
    "frequency_penalty",
    "logit_bias",
    "logprobs",
    "max_tokens",
    "min_p",
    "presence_penalty",
    "repetition_penalty",
    "seed",
    "stop",
    "temperature",
    "top_k",
    "top_logprobs",
    "top_p"
  ],
  "inputs": [
    {
      "name": "system_prompt",
      "display_name": "System Prompt",
      "type": "string or message",
      "description": "System prompt for the model",
      "default": null
    },
    {
      "name": "messages",
      "display_name": "Messages",
      "type": "array of messages or message or string",
      "description": "Array of chat messages",
      "required": true
    },
    {
      "name": "temperature",
      "display_name": "Temperature",
      "type": "number",
      "description": "Controls randomness (0-2)",
      "default": null
    },
    {
      "name": "max_tokens",
      "display_name": "Max Tokens",
      "type": "number",
      "description": "Maximum tokens to generate",
      "default": null
    },
    {
      "name": "top_p",
      "display_name": "Top P",
      "type": "number",
      "description": "Controls diversity via nucleus sampling",
      "default": null
    },
    {
      "name": "frequency_penalty",
      "display_name": "Frequency Penalty",
      "type": "number",
      "description": "Reduces repetition (-2 to 2)",
      "default": null
    },
    {
      "name": "presence_penalty",
      "display_name": "Presence Penalty",
      "type": "number",
      "description": "Encourages new topics (-2 to 2)",
      "default": null
    },
    {
      "name": "seed",
      "display_name": "Seed",
      "type": "number",
      "description": "Deterministic outputs",
      "default": null
    },
    {
      "name": "stop",
      "display_name": "Stop",
      "type": "string or array",
      "description": "Custom stop sequences",
      "default": null
    }
  ],
  "outputs": [
    {
      "name": "content",
      "display_name": "Content",
      "can_stream": true,
      "type": "string",
      "description": "The content portion of the generated response message."
    },
    {
      "name": "message",
      "display_name": "Message",
      "type": "message",
      "can_stream": true,
      "description": "The full generated response message."
    },
    {
      "name": "role",
      "display_name": "Role",
      "can_stream": true,
      "type": "string",
      "description": "Role of the response (usually 'assistant')"
    },
    {
      "name": "finish_reason",
      "display_name": "Finish Reason",
      "type": "string",
      "description": "Why the completion finished"
    },
    {
      "name": "usage",
      "display_name": "Token Usage",
      "type": "object",
      "description": "Token usage statistics"
    },
    {
      "name": "cost_total",
      "display_name": "Total Cost",
      "type": "number",
      "description": "Total cost for processing this request (USD)"
    },
    {
      "name": "cost_itemized",
      "display_name": "Itemized Cost",
      "type": "array",
      "description": "Detailed breakdown of costs"
    },
    {
      "name": "logprobs",
      "display_name": "Log Probabilities",
      "type": "object",
      "description": "Token probabilities and logprobs from the model"
    }
  ],
  "pricing": {
    "reference": "https://openrouter.ai/models",
    "items": [
      {
        "key": "input_cost_per_million",
        "label": "Input Tokens (per 1M)",
        "cost": 0,
        "currency": "USD"
      },
      {
        "key": "output_cost_per_million",
        "label": "Output Tokens (per 1M)",
        "cost": 0,
        "currency": "USD"
      }
    ]
  }
}