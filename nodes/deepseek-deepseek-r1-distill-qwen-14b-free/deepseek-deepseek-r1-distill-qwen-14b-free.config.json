{
  "display_name": "DeepSeek: R1 Distill Qwen 14B (free)",
  "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 69.7\n- MATH-500 pass@1: 93.9\n- CodeForces Rating: 1481\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
  "category": "llm",
  "provider": "deepseek",
  "accepts_plugins": false,
  "model_id": "deepseek/deepseek-r1-distill-qwen-14b:free",
  "context_length": 64000,
  "supported_parameters": [
    "frequency_penalty",
    "include_reasoning",
    "logit_bias",
    "logprobs",
    "max_tokens",
    "min_p",
    "presence_penalty",
    "reasoning",
    "repetition_penalty",
    "seed",
    "stop",
    "temperature",
    "top_k",
    "top_logprobs",
    "top_p"
  ],
  "inputs": [
    {
      "name": "system_prompt",
      "display_name": "System Prompt",
      "type": "string or message",
      "description": "System prompt for the model",
      "default": null
    },
    {
      "name": "messages",
      "display_name": "Messages",
      "type": "array of messages or message or string",
      "description": "Array of chat messages",
      "required": true
    },
    {
      "name": "temperature",
      "display_name": "Temperature",
      "type": "number",
      "description": "Controls randomness (0-2)",
      "default": null
    },
    {
      "name": "max_tokens",
      "display_name": "Max Tokens",
      "type": "number",
      "description": "Maximum tokens to generate",
      "default": null
    },
    {
      "name": "top_p",
      "display_name": "Top P",
      "type": "number",
      "description": "Controls diversity via nucleus sampling",
      "default": null
    },
    {
      "name": "frequency_penalty",
      "display_name": "Frequency Penalty",
      "type": "number",
      "description": "Reduces repetition (-2 to 2)",
      "default": null
    },
    {
      "name": "include_reasoning",
      "display_name": "Include Reasoning",
      "type": "boolean",
      "description": "Include reasoning in response",
      "default": null
    },
    {
      "name": "presence_penalty",
      "display_name": "Presence Penalty",
      "type": "number",
      "description": "Encourages new topics (-2 to 2)",
      "default": null
    },
    {
      "name": "reasoning",
      "display_name": "Reasoning",
      "type": "boolean",
      "description": "Internal reasoning mode",
      "default": null
    },
    {
      "name": "seed",
      "display_name": "Seed",
      "type": "number",
      "description": "Deterministic outputs",
      "default": null
    },
    {
      "name": "stop",
      "display_name": "Stop",
      "type": "string or array",
      "description": "Custom stop sequences",
      "default": null
    }
  ],
  "outputs": [
    {
      "name": "content",
      "display_name": "Content",
      "can_stream": true,
      "type": "string",
      "description": "The content portion of the generated response message."
    },
    {
      "name": "message",
      "display_name": "Message",
      "type": "message",
      "can_stream": true,
      "description": "The full generated response message."
    },
    {
      "name": "role",
      "display_name": "Role",
      "can_stream": true,
      "type": "string",
      "description": "Role of the response (usually 'assistant')"
    },
    {
      "name": "finish_reason",
      "display_name": "Finish Reason",
      "type": "string",
      "description": "Why the completion finished"
    },
    {
      "name": "usage",
      "display_name": "Token Usage",
      "type": "object",
      "description": "Token usage statistics"
    },
    {
      "name": "cost_total",
      "display_name": "Total Cost",
      "type": "number",
      "description": "Total cost for processing this request (USD)"
    },
    {
      "name": "cost_itemized",
      "display_name": "Itemized Cost",
      "type": "array",
      "description": "Detailed breakdown of costs"
    },
    {
      "name": "logprobs",
      "display_name": "Log Probabilities",
      "type": "object",
      "description": "Token probabilities and logprobs from the model"
    },
    {
      "name": "reasoning",
      "display_name": "Reasoning",
      "type": "string",
      "description": "The detailed reasoning chain from the model"
    },
    {
      "name": "refusal",
      "display_name": "Refusal",
      "type": "string",
      "description": "Model refusal response (if any)"
    }
  ],
  "pricing": {
    "reference": "https://openrouter.ai/models",
    "items": [
      {
        "key": "input_cost_per_million",
        "label": "Input Tokens (per 1M)",
        "cost": 0,
        "currency": "USD"
      },
      {
        "key": "output_cost_per_million",
        "label": "Output Tokens (per 1M)",
        "cost": 0,
        "currency": "USD"
      }
    ]
  }
}