{
  "display_name": "xAI: Grok 4",
  "description": "Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs. Note that reasoning is not exposed, reasoning cannot be disabled, and the reasoning effort cannot be specified. Pricing increases once the total tokens in a given request is greater than 128k tokens. See more details on the [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)",
  "category": "llm",
  "provider": "x-ai",
  "accepts_plugins": true,
  "model_id": "x-ai/grok-4",
  "context_length": 256000,
  "supported_parameters": [
    "include_reasoning",
    "logprobs",
    "max_tokens",
    "reasoning",
    "response_format",
    "seed",
    "structured_outputs",
    "temperature",
    "tool_choice",
    "tools",
    "top_logprobs",
    "top_p"
  ],
  "inputs": [
    {
      "name": "system_prompt",
      "display_name": "System Prompt",
      "type": "string or message",
      "description": "System prompt for the model",
      "default": null
    },
    {
      "name": "messages",
      "display_name": "Messages",
      "type": "array of messages or message or string",
      "description": "Array of chat messages",
      "required": true
    },
    {
      "name": "tools",
      "display_name": "Tools",
      "type": "tool",
      "description": "Array of tools to use",
      "default": null,
      "allow_multiple": true
    },
    {
      "name": "temperature",
      "display_name": "Temperature",
      "type": "number",
      "description": "Controls randomness (0-2)",
      "default": null
    },
    {
      "name": "max_tokens",
      "display_name": "Max Tokens",
      "type": "number",
      "description": "Maximum tokens to generate",
      "default": null
    },
    {
      "name": "top_p",
      "display_name": "Top P",
      "type": "number",
      "description": "Controls diversity via nucleus sampling",
      "default": null
    },
    {
      "name": "include_reasoning",
      "display_name": "Include Reasoning",
      "type": "boolean",
      "description": "Include reasoning in response",
      "default": null
    },
    {
      "name": "reasoning",
      "display_name": "Reasoning",
      "type": "boolean",
      "description": "Internal reasoning mode",
      "default": null
    },
    {
      "name": "response_format",
      "display_name": "Response Format",
      "type": "string or object",
      "description": "Output format specification",
      "default": null
    },
    {
      "name": "seed",
      "display_name": "Seed",
      "type": "number",
      "description": "Deterministic outputs",
      "default": null
    },
    {
      "name": "structured_outputs",
      "display_name": "Structured Outputs",
      "type": "string or object",
      "description": "JSON schema enforcement",
      "default": null
    },
    {
      "name": "tool_choice",
      "display_name": "Tool Choice",
      "type": "string",
      "description": "Tool selection control",
      "default": null
    }
  ],
  "outputs": [
    {
      "name": "content",
      "display_name": "Content",
      "can_stream": true,
      "type": "string",
      "description": "The content portion of the generated response message."
    },
    {
      "name": "message",
      "display_name": "Message",
      "type": "message",
      "can_stream": true,
      "description": "The full generated response message."
    },
    {
      "name": "role",
      "display_name": "Role",
      "can_stream": true,
      "type": "string",
      "description": "Role of the response (usually 'assistant')"
    },
    {
      "name": "tool_calls",
      "display_name": "Tool Calls",
      "type": "array of tools",
      "description": "Tool calls made by the model"
    },
    {
      "name": "finish_reason",
      "display_name": "Finish Reason",
      "type": "string",
      "description": "Why the completion finished"
    },
    {
      "name": "usage",
      "display_name": "Token Usage",
      "type": "object",
      "description": "Token usage statistics"
    },
    {
      "name": "cost_total",
      "display_name": "Total Cost",
      "type": "number",
      "description": "Total cost for processing this request (USD)"
    },
    {
      "name": "cost_itemized",
      "display_name": "Itemized Cost",
      "type": "array",
      "description": "Detailed breakdown of costs"
    },
    {
      "name": "logprobs",
      "display_name": "Log Probabilities",
      "type": "object",
      "description": "Token probabilities and logprobs from the model"
    },
    {
      "name": "reasoning",
      "display_name": "Reasoning",
      "type": "string",
      "description": "The detailed reasoning chain from the model"
    },
    {
      "name": "refusal",
      "display_name": "Refusal",
      "type": "string",
      "description": "Model refusal response (if any)"
    }
  ],
  "pricing": {
    "reference": "https://openrouter.ai/models",
    "items": [
      {
        "key": "input_cost_per_million",
        "label": "Input Tokens (per 1M)",
        "cost": 3,
        "currency": "USD"
      },
      {
        "key": "output_cost_per_million",
        "label": "Output Tokens (per 1M)",
        "cost": 15,
        "currency": "USD"
      }
    ]
  }
}