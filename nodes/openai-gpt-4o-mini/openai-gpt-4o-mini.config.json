{
  "display_name": "OpenAI: GPT-4o-mini",
  "description": "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
  "category": "llm",
  "provider": "openai",
  "accepts_plugins": true,
  "model_id": "openai/gpt-4o-mini",
  "context_length": 128000,
  "supported_parameters": [
    "frequency_penalty",
    "logit_bias",
    "logprobs",
    "max_tokens",
    "presence_penalty",
    "response_format",
    "seed",
    "stop",
    "structured_outputs",
    "temperature",
    "tool_choice",
    "tools",
    "top_logprobs",
    "top_p",
    "web_search_options"
  ],
  "inputs": [
    {
      "name": "system_prompt",
      "display_name": "System Prompt",
      "type": "string or message",
      "description": "System prompt for the model",
      "default": null
    },
    {
      "name": "messages",
      "display_name": "Messages",
      "type": "array of messages or message or string",
      "description": "Array of chat messages",
      "required": true
    },
    {
      "name": "tools",
      "display_name": "Tools",
      "type": "tool",
      "description": "Array of tools to use",
      "default": null,
      "allow_multiple": true
    },
    {
      "name": "temperature",
      "display_name": "Temperature",
      "type": "number",
      "description": "Controls randomness (0-2)",
      "default": null
    },
    {
      "name": "max_tokens",
      "display_name": "Max Tokens",
      "type": "number",
      "description": "Maximum tokens to generate",
      "default": null
    },
    {
      "name": "top_p",
      "display_name": "Top P",
      "type": "number",
      "description": "Controls diversity via nucleus sampling",
      "default": null
    },
    {
      "name": "frequency_penalty",
      "display_name": "Frequency Penalty",
      "type": "number",
      "description": "Reduces repetition (-2 to 2)",
      "default": null
    },
    {
      "name": "presence_penalty",
      "display_name": "Presence Penalty",
      "type": "number",
      "description": "Encourages new topics (-2 to 2)",
      "default": null
    },
    {
      "name": "response_format",
      "display_name": "Response Format",
      "type": "string or object",
      "description": "Output format specification",
      "default": null
    },
    {
      "name": "seed",
      "display_name": "Seed",
      "type": "number",
      "description": "Deterministic outputs",
      "default": null
    },
    {
      "name": "stop",
      "display_name": "Stop",
      "type": "string or array",
      "description": "Custom stop sequences",
      "default": null
    },
    {
      "name": "structured_outputs",
      "display_name": "Structured Outputs",
      "type": "string or object",
      "description": "JSON schema enforcement",
      "default": null
    },
    {
      "name": "tool_choice",
      "display_name": "Tool Choice",
      "type": "string",
      "description": "Tool selection control",
      "default": null
    }
  ],
  "outputs": [
    {
      "name": "content",
      "display_name": "Content",
      "can_stream": true,
      "type": "string",
      "description": "The content portion of the generated response message."
    },
    {
      "name": "message",
      "display_name": "Message",
      "type": "message",
      "can_stream": true,
      "description": "The full generated response message."
    },
    {
      "name": "role",
      "display_name": "Role",
      "can_stream": true,
      "type": "string",
      "description": "Role of the response (usually 'assistant')"
    },
    {
      "name": "tool_calls",
      "display_name": "Tool Calls",
      "type": "array of tools",
      "description": "Tool calls made by the model"
    },
    {
      "name": "finish_reason",
      "display_name": "Finish Reason",
      "type": "string",
      "description": "Why the completion finished"
    },
    {
      "name": "usage",
      "display_name": "Token Usage",
      "type": "object",
      "description": "Token usage statistics"
    },
    {
      "name": "cost_total",
      "display_name": "Total Cost",
      "type": "number",
      "description": "Total cost for processing this request (USD)"
    },
    {
      "name": "cost_itemized",
      "display_name": "Itemized Cost",
      "type": "array",
      "description": "Detailed breakdown of costs"
    },
    {
      "name": "annotations",
      "display_name": "Annotations",
      "type": "array",
      "description": "Array of annotations and citations from the response"
    },
    {
      "name": "citations",
      "display_name": "Citations",
      "type": "array",
      "description": "Array of citation URLs used by the model"
    },
    {
      "name": "logprobs",
      "display_name": "Log Probabilities",
      "type": "object",
      "description": "Token probabilities and logprobs from the model"
    }
  ],
  "pricing": {
    "reference": "https://openrouter.ai/models",
    "items": [
      {
        "key": "input_cost_per_million",
        "label": "Input Tokens (per 1M)",
        "cost": 0.15,
        "currency": "USD"
      },
      {
        "key": "output_cost_per_million",
        "label": "Output Tokens (per 1M)",
        "cost": 0.6,
        "currency": "USD"
      }
    ]
  }
}